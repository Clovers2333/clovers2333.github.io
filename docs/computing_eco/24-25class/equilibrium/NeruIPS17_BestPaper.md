# 不完美信息博弈的安全和嵌套子博弈求解 (Safe and Nested Subgame Solving for Imperfect-Information Games) 超详细解析

## 摘要 (Abstract)

*   **核心问题:** 在不完美信息博弈 (Imperfect-Information Games, IIGs) 中，一个**子博弈 (Subgame)** 的最优策略可能依赖于子博弈之外的策略。这意味着子博弈不能被安全地**隔离 (in isolation)** 求解，必须将整个游戏的策略考虑在内。

*   **现状:** 对于大型 IIGs，通常先近似求解整个游戏，得到一个"**蓝图 (blueprint)"策略。然后，在游戏实际进行到某个子博弈时，可以实时地进一步求解该子博弈以改进策略。这种方法被称为**子博弈求解 (Subgame Solving)**。

*   **本文贡献:**
    1.  引入并改进了多种子博弈求解技术，这些技术对先前的方法进行了调整，以适应理论和实践。
    2.  展示了如何使这些技术**安全 (safe)**，即保证通过子博弈求解更新后的策略，其**可利用性 (exploitability)** 不会比原始蓝图策略更高。
    3.  提出了**嵌套子博弈求解 (Nested Subgame Solving)** 技术，用于响应对手采取的、在原始行动抽象之外的行动（**行动转换 Action Translation**）。
    4.  实验表明，这些方法显著优于之前的最新技术，大幅降低了策略的可利用性。
    5.  这些技术是 AI 程序 **Libratus** 在德州扑克中击败顶尖人类玩家的关键组成部分。

## 第 1 章：引言 (Introduction)

*   **背景:** 不完美信息博弈模拟了许多现实世界的战略互动，其中参与者拥有隐藏信息。这与完美信息博弈（如象棋、围棋）形成对比，后者的所有信息都是公开的。

*   **完美信息博弈的子博弈完美性:** 在完美信息博弈中，可以仅基于当前节点状态和未来的子博弈来确定最优策略（**子博弈完美均衡 Subgame Perfect Equilibrium**）。子博弈可以被独立分析和求解。这是 AlphaGo 等 AI 成功的基础。

*   **不完美信息博弈的挑战:** 子博弈的概念在 IIGs 中更为复杂。一个节点的"最优"行动不仅取决于该节点之后的游戏状态，还取决于该节点之前的历史如何影响了玩家对当前状态（特别是对手隐藏信息）的**信念 (beliefs)**。对手在游戏早期的行动会影响我们在子博弈中对其可能手牌的判断。因此，**子博弈不能被完美地分解和独立求解**。

*   **大规模博弈的复杂性:** 现实世界的 IIGs（如扑克）状态空间极其巨大。例如，双人无限注德州扑克大约有 \(10^{161}\) 个决策点，远超宇宙中的原子数量。这使得直接求解精确的纳什均衡 (NE) 策略变得不可能。

*   **标准的三步方法:**
    1.  **抽象 (Abstraction):** 创建一个规模较小、但保留了原游戏战略复杂性的简化版游戏。这可能涉及将相似的手牌归入"桶 (buckets)"中，或限制允许的下注大小。
    2.  **蓝图策略计算:** 在这个抽象游戏中计算一个近似的纳什均衡策略（**蓝图策略 Blueprint Strategy**）。常用的算法是**反事实后悔最小化 (Counterfactual Regret Minimization, CFR)** 及其变种。
    3.  **子博弈求解:** 在实际游戏对局中，当游戏进行到某个子博弈（例如，德州扑克的翻牌圈之后）时，利用额外的计算时间，针对当前遇到的具体公共信息和玩家的精确手牌（如果已知），"实时"地计算一个比蓝图策略更精细、更优化的策略。这个过程只更新当前子博弈内的策略。

*   **本文目标:** 深入研究和改进"子博弈求解"这一环节。提出新的子博弈求解技术，特别是保证其"安全性"的方法，并开发"嵌套子博弈求解"来处理对手的意外行动（行动转换）。

## 第 2 章：抛硬币游戏 (Coin Toss)

*   **目的:** 使用一个极简的 IIG 示例，直观地说明为何子博弈不能孤立求解。

*   **游戏设置 (图 1a):**
    *   两个玩家 P1, P2。初始为机会节点 C，以等概率 \(p=0.5\) 选择 H (正面) 或 T (反面)。
    *   P1 观察到结果 (H 或 T)，但 P2 不知道。
    *   P1 选择行动：**卖出 (Sell)** 或 **玩 (Play)**。
        *   若 P1 选 Sell：如果结果是 H，P1 获得 \(\$0.50\)；如果结果是 T，P1 获得 \(-\$0.50\)。游戏结束。
        *   若 P1 选 Play：游戏继续，轮到 P2 行动。
    *   P2 知道 P1 选择了 Play，但仍然不知道硬币是 H 还是 T。这两个节点构成了 P2 的一个**信息集 (Infoset)** (用虚线连接)。P2 选择行动：**猜 H (Heads)** 或 **猜 T (Tails)**。
        *   若 P2 猜对（H 时猜 H，T 时猜 T）：P2 赢得 \(\$1\)，P1 损失 \(\$1\)。
        *   若 P2 猜错（H 时猜 T，T 时猜 H）：P2 损失 \(\$1\)，P1 赢得 \(\$1\)。

*   **孤立子博弈分析 (错误):** 考虑 P2 做决策的子博弈 S（即 P1 选择 Play 后的部分）。如果 P2 **假设** P1 在 H 和 T 时都以相同的概率进入这个子博弈，那么对于 P2：
    *   猜 H 的期望收益 (EV): \(0.5 \times u_2(\text{H, Play, Heads}) + 0.5 \times u_2(\text{T, Play, Heads}) = 0.5 \times (-1) + 0.5 \times 1 = 0\)。
    *   猜 T 的期望收益 (EV): \(0.5 \times u_2(\text{H, Play, Tails}) + 0.5 \times u_2(\text{T, Play, Tails}) = 0.5 \times 1 + 0.5 \times (-1) = 0\)。
    在这个错误的假设下，P2 似乎无论猜 H 还是 T 都是无所谓的。如果 P2 决定总是猜 T，那么 P1 在这个子博弈中的 EV 是 \(0.5 \times u_1(\text{H, Play, Tails}) + 0.5 \times u_1(\text{T, Play, Tails}) = 0.5 \times (-1) + 0.5 \times 1 = 0\)。

*   **整体游戏分析 (正确):** P2 的最优策略必须考虑 P1 在子博弈 S **之外**的选择 (Sell vs Play)。
    *   假设 P2 真的总是猜 T。那么 P1 的决策：
        *   若结果为 H：选 Sell (EV=0.5) vs 选 Play (P2猜T, P1 EV= -1)。P1 应选 Sell。
        *   若结果为 T：选 Sell (EV=-0.5) vs 选 Play (P2猜T, P1 EV= 1)。P1 应选 Play。
    *   因此，P1 的策略是 (H -> Sell, T -> Play)。P1 的总期望收益是 \(0.5 \times 0.5 + 0.5 \times 1 = 0.75\)。
    *   现在 P2 预见到 P1 的策略。当 P1 选择 Play 时，P2 **确信**硬币结果是 T。因此，P2 的最佳反应是总是猜 T。这似乎是一个稳定点。
    *   **但是！** 这个 (P1: H->Sell, T->Play; P2: Always Guess T) 并非纳什均衡。P1 的 EV 是 0.75，P2 的 EV 是 -0.75。P2 有动机偏离 "Always Guess T"。
    *   考虑 P2 的混合策略：以概率 \(q\) 猜 H，以概率 \(1-q\) 猜 T。
        *   P1 在 H 时：Sell (0.5) vs Play (EV = \(q \times (-1) + (1-q) \times 1 = 1-2q\))。P1 选择 Sell 如果 \(0.5 > 1-2q\)，即 \(q > 0.25\)。
        *   P1 在 T 时：Sell (-0.5) vs Play (EV = \(q \times 1 + (1-q) \times (-1) = 2q-1\))。P1 选择 Play 如果 \(2q-1 > -0.5\)，即 \(q > 0.25\)。
    *   为了让 P1 在 H 和 T 时都愿意混合策略（或至少有一个愿意），P2 需要让 P1 在 Play 和 Sell 之间无差别。
        *   在 H 时无差别：\(0.5 = 1-2q \implies q = 0.25\)。
        *   在 T 时无差别：\(-0.5 = 2q-1 \implies q = 0.25\)。
    *   所以 P2 的均衡策略是：\(q=0.25\) 猜 H，\(1-q=0.75\) 猜 T。
    *   当 P2 采用此策略时，P1 的决策：
        *   在 H 时：Sell (0.5) vs Play (EV = \(1 - 2 \times 0.25 = 0.5\))。P1 无差别。
        *   在 T 时：Sell (-0.5) vs Play (EV = \(2 \times 0.25 - 1 = -0.5\))。P1 无差别。
    *   P1 的均衡策略可以是 H 时总是 Sell，T 时总是 Play (因为 Play 收益不低于 Sell)。
    *   所以，一个纳什均衡是：P1 采取 (H -> Sell, T -> Play)，P2 采取 (25% 猜 H, 75% 猜 T)。此时 P1 的总期望收益是 \(0.5 \times 0.5 + 0.5 \times (-0.5) = 0\)。这与之前分析的 0.75 不同，说明之前的分析未达均衡。另一个 NE 是 P1: (H -> 50% Sell, 50% Play), (T -> Play); P2: (25% H, 75% T)。
    *   **关键点:** P2 在子博弈 S 中的最优策略（25% H, 75% T）是由 P1 在 S 之外的行为（特别是 P1 在 H 时选择 Sell）决定的。孤立地看子博弈 S 并假设均匀进入，会得出错误的结论（例如 P2 随便猜或总是猜 T）。

*   **结论:** 这个简单的例子清晰地表明，**不完美信息博弈中的子博弈策略高度依赖于子博弈之外的策略**。这是子博弈求解的核心挑战。

## 第 3 章：符号和背景 (Notation and Background)

本章建立形式化定义。

*   **扩展式博弈 (Extensive-Form Game):** 一个扩展式博弈定义为：
    *   **历史/节点 (Histories/Nodes) \(H\):** 一个有限的节点集合，构成一个博弈树。包含非终结节点和终结 (叶子) 节点 \(Z \subseteq H\)。根节点是空历史。
    *   **行动玩家 (Player Function) \(P: H \setminus Z \to \{1, ..., n, c\}\):** 指定在每个非终结节点 \(h\) 行动的玩家 (1 到 n) 或机会玩家 (c)。
    *   **动作 (Actions) \(A(h)\):** 在节点 \(h\) 可选的有限动作集合。\(A(h) = \emptyset\) 当且仅当 \(h \in Z\)。
    *   **后继节点 (Successor Function):** 对于 \(h \in H \setminus Z\) 和 \(a \in A(h)\)，\(h \cdot a\) 表示采取动作 \(a\) 后到达的节点。
    *   **机会概率 (Chance Probabilities) \(p(h, a)\):** 如果 \(P(h)=c\)，则 \(p(h, \cdot)\) 是 \(A(h)\) 上的概率分布，且 \(\sum_{a \in A(h)} p(h, a) = 1\)。
    *   **玩家收益 (Player Payoffs) \(u_i: Z \to \mathbb{R}\):** 每个玩家 \(i\) 在每个终结节点 \(z\) 获得的收益。本文主要关注**双人零和博弈 (two-player zero-sum games)**，即 \(n=2\) 且对所有 \(z \in Z\)，\(u_1(z) = -u_2(z)\)。

*   **不完美信息 (Imperfect Information):**
    *   **信息集 (Infosets) \(I_i\):** 玩家 \(i\) 的信息集 \(I_i\) 是节点集合 \(H_i = \{h \in H | P(h)=i\}\) 的一个划分。同一信息集 \(I \in I_i\) 中的所有节点 \(h, h' \in I\) 对玩家 \(i\) 来说是不可区分的。这意味着：
        1.  在这些节点上行动的是同一个玩家：\(P(h) = P(h') = i\)。
        2.  在这些节点上可用的动作集合必须相同：\(A(h) = A(h')\)。我们用 \(A(I)\) 表示这个共同的动作集。
    *   如果每个信息集都只包含一个节点，则为完美信息博弈。

*   **策略 (Strategy):**
    *   玩家 \(i\) 的一个（行为）策略 \(\sigma_i\) 是一个映射，为每个信息集 \(I \in I_i\) 指定一个在动作 \(A(I)\) 上的概率分布 \(\sigma_i(I)\)。
    *   \(\sigma_i(I, a)\) 表示玩家 \(i\) 在信息集 \(I\) 选择动作 \(a \in A(I)\) 的概率。要求 \(\sum_{a \in A(I)} \sigma_i(I, a) = 1\)。
    *   **策略组合 (Strategy Profile) \(\sigma = (\sigma_1, ..., \sigma_n)\):** 包含所有玩家策略的组合。
    *   \(\sigma_{-i}\) 表示除玩家 \(i\) 之外所有玩家的策略组合。\(\sigma = (\sigma_i, \sigma_{-i})\)。

*   **到达概率 (Reach Probability):**
    *   \(\pi^\sigma(h)\): 在策略组合 \(\sigma\) 下，游戏进行到节点 \(h\) 的总概率。它是从根节点到 \(h\) 的路径上所有机会概率和玩家选择相应动作的概率的乘积。
    *   \(\pi^\sigma_i(h)\): 只考虑玩家 \(i\) 的行动对到达 \(h\) 的贡献概率。即路径上所有玩家 \(i\) 的动作概率的乘积。
    *   \(\pi^\sigma_{-i}(h)\): 只考虑除玩家 \(i\) 外所有玩家（包括机会玩家 c）的行动对到达 \(h\) 的贡献概率。
    *   对于任何 \(h\)，有 \(\pi^\sigma(h) = \pi^\sigma_i(h) \pi^\sigma_{-i}(h)\)。
    *   \(\pi^\sigma(I) = \sum_{h \in I} \pi^\sigma(h)\): 到达信息集 \(I \in I_i\) 的概率。
    *   \(\pi^\sigma(h|I) = \frac{\pi^\sigma(h)}{\pi^\sigma(I)}\): 假定游戏到达了信息集 \(I\)，当前实际处于节点 \(h \in I\) 的条件概率（玩家 \(i\) 的信念）。需要 \(\pi^\sigma(I) > 0\)。

*   **期望收益 (Expected Value):**
    *   \(v_i(h, \sigma)\): 从节点 \(h\) 开始，所有玩家都按照策略组合 \(\sigma\) 继续游戏，玩家 \(i\) 能获得的期望收益。计算方法是从 \(h\) 出发，对所有可能到达的终结节点 \(z\)，用到达 \(z\) 的条件概率乘以 \(u_i(z)\) 再求和。
    *   \(v_i(\sigma) = v_i(\text{root}, \sigma) = \sum_{z \in Z} \pi^\sigma(z) u_i(z)\): 整个游戏从根节点开始，玩家 \(i\) 的总期望收益。
    *   \(v_i(I, \sigma) = \sum_{h \in I} \pi^\sigma(h|I) v_i(h, \sigma) = \frac{\sum_{h \in I} \pi^\sigma(h) v_i(h, \sigma)}{\pi^\sigma(I)}\): 玩家 \(i\) 在其信息集 \(I \in I_i\) 上的期望收益（假定 \(\pi^\sigma(I) > 0\)）。

*   **纳什均衡 (Nash Equilibrium - NE):**
    *   一个策略组合 \(\sigma^* = (\sigma^*_1, ..., \sigma^*_n)\) 是纳什均衡，如果对于任何玩家 \(i\) 和他的任何其他策略 \(\sigma'_i\)，都有：
        \[
        v_i(\sigma^*) \ge v_i(\sigma'_i, \sigma^*_{-i})
        \]
        即没有任何玩家可以通过单方面改变自己的策略来提高期望收益。
    *   **最佳响应 (Best Response - BR):** 策略 \(\sigma^*_i\) 是对 \(\sigma^*_{-i}\) 的最佳响应，如果它最大化了玩家 \(i\) 的期望收益：\(\sigma^*_i \in BR(\sigma^*_{-i}) = \arg\max_{\sigma'_i} v_i(\sigma'_i, \sigma^*_{-i})\)。NE 是所有玩家策略互为最佳响应的策略组合。
    *   **可利用性 (Exploitability):** 衡量一个策略组合 \(\sigma\) 偏离 NE 的程度。对于双人零和博弈，定义为：
  
        \[
        \begin{align*}
        expl(\sigma) &= \max_{\sigma'_1} v_1(\sigma'_1, \sigma_2) + \max_{\sigma'_2} v_2(\sigma_1, \sigma'_2) \\\\
        &= \max_{\sigma'_1} v_1(\sigma'_1, \sigma_2) - \min_{\sigma'_2} v_1(\sigma_1, \sigma'_2) \\\\
        &= (v_1(BR(\sigma_2), \sigma_2) - v_1(\sigma_1, \sigma_2)) + (v_2(\sigma_1, BR(\sigma_1)) - v_2(\sigma_1, \sigma_2))
        \end{align*}
        \]

        它等于两个玩家各自的最佳响应收益之和（因为是零和博弈，\(v_2 = -v_1\)，所以第二个 max 变成 -min）。或者说，是每个玩家通过切换到最佳响应策略能够获得的收益改进量之和。NE 的可利用性为 0。可利用性越低，策略越接近均衡。

*   **反事实值 (Counterfactual Value - CFV):** 这是 CFR 算法的核心概念。
    *   假设我们固定除玩家 \(i\) 之外所有玩家的策略 \(\sigma_{-i}\)。想象玩家 \(i\) **总是试图**到达某个节点 \(h\)（即，从根到 \(h\) 的路径上，所有属于玩家 \(i\) 的动作都被选择）。在这种反事实情景下，从 \(h\) 开始游戏，玩家 \(i\) 的期望收益是多少？这个值由 \(\pi^\sigma_{-i}(h) v_i(h, \sigma)\) 给出。
  
    *   **信息集 \(I \in I_i\) 的反事实值 (Counterfactual Value of an Infoset):**
  
        \[
        v^\sigma(I) = \sum_{h \in I} \pi^\sigma_{-i}(h) v_i(h, \sigma)
        \]

        它是在假设玩家 \(i\) 总是试图到达信息集 \(I\) 的条件下，玩家 \(i\) 在该信息集上的期望收益，权重由其他玩家的到达概率 \(\pi^\sigma_{-i}(h)\) 决定。

        这是采取动作 \(a\) 后，在后继节点 \(h \cdot a\) 开始游戏的期望收益，同样用 \(\pi^\sigma_{-i}(h)\) 加权。

    *   关系：\(v^\sigma(I) = \sum_{a \in A(I)} \sigma_i(I, a) v^\sigma(I, a)\)。

*   **反事实最佳响应 (Counterfactual Best Response - CBR):**
    *   在信息集 \(I \in I_i\) 对策略 \(\sigma\) 的反事实最佳响应动作是最大化 \(v^\sigma(I, a)\) 的动作 \(a^* \in A(I)\)。
    *   \(CBR^\sigma(I) = \arg\max_{a \in A(I)} v^\sigma(I, a)\) (可能是一个集合)。

*   **反事实最佳值 (Counterfactual Best Value - CBV):**
    *   玩家 \(i\) 在信息集 \(I \in I_i\) 能达到的最大反事实值：
  
        \[
        CBV^\sigma(I) = \max_{a \in A(I)} v^\sigma(I, a)
        \]

*   **不完美信息子博弈 (Imperfect-Information Subgame):**
    *   一个节点集合 \(S \subseteq H\) 定义了一个不完美信息子博弈，如果满足以下条件：
        1.  **后继闭包 (Successor Closure):** 如果 \(h \in S\) 且 \(h \notin Z\)，那么对于所有 \(a \in A(h)\)，\(h \cdot a \in S\)。即子博弈包含了其内部节点的所有后代。
        2.  **信息集闭包 (Infoset Closure):** 如果 \(h \in S\) 且 \(h\) 属于某个玩家 \(i\) 的信息集 \(I \in I_i\)，那么整个信息集 \(I\) 都必须包含在 \(S\) 中（\(I \subseteq S\)）。这确保了进入子博弈的玩家知道自己确实在子博弈内。
    *   \(S_{top}\) 是子博弈 \(S\) 中"最早"的节点集合：\(S_{top} = \{h \in S \mid \text{父节点}(h) \notin S\}\)。这些是进入子博弈的入口点。

## 第 4 章：先前的子博弈求解方法 (Prior Approaches to Subgame Solving)

*   **目标:** 给定一个（通常是抽象游戏上计算出的）蓝图策略 \(\sigma\)，以及一个不完美信息子博弈 \(S\)。我们想在保持对手策略 \(\sigma_1\) 不变的情况下，计算玩家 P2 在子博弈 \(S\) 内的新策略 \(\sigma'_2\)，使得最终的全局策略 \(\sigma' = (\sigma_1, \sigma'_2)\) 比原始策略 \(\sigma = (\sigma_1, \sigma_2)\) 更好（通常意味着更低的**可利用性**）。

*   **通用框架:**
    1.  **构建增广子博弈 (Augmented Subgame):**
        *   这个增广博弈的根节点是一个新的机会节点。
        *   这个机会节点的子节点对应于原始子博弈的入口节点集合 \(S_{top}\)。
        *   从新根节点到某个 \(h \in S_{top}\) 的转移概率，正比于在原始蓝图策略 \(\sigma\) 下，**除 P2 之外**的所有玩家（即 P1 和机会玩家 c）使得游戏到达节点 \(h\) 的概率 \(\pi^\sigma_{-P_2}(h)\)。概率需要归一化，使得 \(\sum_{h \in S_{top}} p(\text{new root}, h) = 1\)。
        *   从每个 \(h \in S_{top}\) 开始，游戏结构与原始子博弈 \(S\) 内部相同。
        *   这个增广博弈捕捉了 P2 进入子博弈 S 时面临的关于具体入口状态的不确定性。
    2.  **求解增广子博弈:** 在这个增广子博弈中，针对 P2 计算一个策略。不同的方法在这一步有所不同。
    3.  **更新策略:** 将计算出的 P2 策略 \(\sigma'_2\)（仅限于 S 内部的信息集）替换掉原始蓝图策略 \(\sigma\) 中对应的部分，得到最终策略 \(\sigma'\)。

*   **4.1 不安全子博弈求解 (Unsafe Subgame Solving) (参考图 3a):**
    *   **方法:** 在上述构建的增广子博弈中，直接计算一个**纳什均衡 (NE)**（例如，使用 CFR 算法）。然后提取这个 NE 中 P2 的策略部分作为 \(\sigma'_2\)。
    *   **问题:** **不安全 (Unsafe)**。最终得到的全局策略 \(\sigma' = (\sigma_1, \sigma'_2)\) 的可利用性 \(expl(\sigma')\) **可能高于** 原始蓝图策略 \(\sigma\) 的可利用性 \(expl(\sigma)\)。
    *   **原因:** 求解增广子博弈时，它隐式地假设 P1 在子博弈 S **内部和外部**都会固定使用 \(\sigma_1\)。但当 P2 在 S 内改变策略为 \(\sigma'_2\) 后，P1 的全局最佳响应可能需要在 S **外部**也做出调整来利用 \(\sigma'_2\)。Unsafe 方法没有考虑这种可能性，因此 \(\sigma'_2\) 可能使 P2 在全局范围内更容易被 P1 攻击。在抛硬币例子中，不安全求解得到 P2 策略为 (1/4 猜 H, 3/4 猜 T)，这不是全局最优也不是 NE 策略。

*   **4.2 子博弈决心 (Subgame Resolving) (参考图 3b):**
    *   **方法:** 这是为了解决 Unsafe 方法的不安全性而提出的。
        1.  构建增广子博弈。
        2.  **关键修改:** 对于增广子博弈中那些代表 P1（或机会玩家后紧跟 P1）决策导致进入子博弈 S 的入口节点 \(h \in S_{top}\)，给 P1 增加一个**"退出" (Opt-out / Alternative)** 选项。
        3.  如果 P1 在入口点 \(h\) 选择"退出"，P1 将直接获得一个固定的收益：这个收益等于在**原始蓝图策略 \(\sigma\) 下**，从节点 \(h\) 开始游戏的期望收益 \(v^\sigma_1(h)\)。(P2 则获得 \(v^\sigma_2(h) = -v^\sigma_1(h)\))。
        4.  在这个**修改后的增广子博弈**（包含 P1 的退出选项）中，计算一个纳什均衡 (NE)。
        5.  提取 NE 中 P2 的策略部分作为 \(\sigma'_2\)。
    *   **保证:** **安全 (Safe)**。保证改进后的全局策略 \(\sigma'\) 的可利用性**不会高于**原始蓝图策略 \(\sigma\)。
        \[
        expl(\sigma') \le expl(\sigma)
        \]
    *   **原因:** "退出"选项为 P1 提供了一个**保底收益 (safety net)**。无论 P2 在 S 内采取什么新策略 \(\sigma'_2\)，P1 总可以通过选择"退出"来确保自己至少获得和原策略 \(\sigma\) 一样好的收益。因此，P2 的新策略 \(\sigma'_2\) 不可能通过损害 P1 在子博弈 S 内的利益来获益（相对于 P1 的保底收益而言）。这防止了由于 P2 在 S 内的策略改变而导致全局可利用性的增加。在抛硬币例子中，Resolve 方法正确地让 P2 采取了总是猜 T 的策略（这是该特定蓝图下的安全解，虽然不是全局 NE 解）。

## 第 5 章：到达子博弈求解 (Reach Subgame Solving)

*   **动机:** Resolve 方法保证了安全性，但可能过于保守，因为它只保证不增加可利用性，不一定能显著减少它。Reach Subgame Solving 旨在通过更积极地利用对手策略的弱点来获得更大的改进，同时仍试图提供某种形式的理论保证。它特别关注子博弈的**边界 (boundary)**，即入口点 \(S_{top}\)。

*   **核心思想:** 考虑对手 P1 在进入子博弈 S 的那些信息集 \(I_1 \subseteq S_{top}\) 上的决策。如果 P1 在蓝图策略 \(\sigma_1\) 下在某个 \(I_1\) 采取的行动不是他对 P2 策略 \(\sigma_2\) 的最佳响应，那么 P1 在这个信息集上就存在**后悔值 (regret)** 或者说**边际值 (margin)**。P2 的新策略 \(\sigma'_2\) 应该试图利用或减少 P1 的这些边际值，使得 P1 更没有动机偏离 \(\sigma_1\)。

*   **边际值 (Margin) 定义:** 对于 P1 的一个信息集 \(I_1 \in S_{top}\)，在给定策略组合 \(\sigma'=(\sigma_1, \sigma'_2)\) 下，P1 的边际值定义为：
  
    \[
    M^{\sigma'}(I_1) = CBV^{\sigma'}(I_1) - v^{\sigma'}(I_1)
    \]

    其中 \(CBV^{\sigma'}(I_1) = \max_{a \in A(I_1)} v^{\sigma'}(I_1, a)\) 是 P1 在 \(I_1\) 通过选择最佳动作能达到的最大反事实值，而 \(v^{\sigma'}(I_1) = \sum_{a \in A(I_1)} \sigma_1(I_1, a) v^{\sigma'}(I_1, a)\) 是 P1 按照蓝图策略 \(\sigma_1\) 在 \(I_1\) 的平均反事实值。这个边际值衡量了 P1 在 \(I_1\) 通过偏离 \(\sigma_1\) 转而选择最佳动作可以获得的额外（反事实）收益。

*   **Maxmargin 目标:** 找到 P2 的策略 \(\sigma'_2\)，使得在所有对手的边界信息集 \(I_1 \in S_{top}\) 上，**最小化 P1 的最大边际值**，或者一个相关的目标是**最大化 P1 的最小边际值**，同时要求所有边际值为非负 \(M^{\sigma'}(I_1) \ge 0\)。直观上是试图让 P1 在所有入口处都接近"无差别"状态，减少其偏离 \(\sigma_1\) 的动机。

*   **Reach Margin (公式 1):** 论文中定义了一个 P2 的 **Reach Margin** \(M^{\sigma'}_R(I_1)\)，它结合了基础边际值和一个关于礼物的项（使用下界）：

    \[
    \begin{align}
    M^{\sigma'}_R(I_1) &= M^{\sigma'}_L(I_1) + \sum_{a' \in A(I_1)} \pi^{\sigma'}(a'|I_1) [g^{\sigma', L}_2(I_1, a')] \\
    &\text{其中 } M^{\sigma'}_L(I_1) \text{ 是 } M^{\sigma'}(I_1) \text{ 的一个下界，} \\
    &\pi^{\sigma'}(a'|I_1) \text{ 是在 } \sigma' \text{ 下 P1 在 } I_1 \text{ 选择动作 } a' \text{ 的概率 (即 } \sigma_1(I_1, a')\text{)}。
    \end{align}
    \]

*   **理论保证 (定理 1 - Theorem 1):**
    *   令 \(\sigma = (\sigma_1, \sigma_2)\) 为蓝图策略，\(\sigma' = (\sigma_1, \sigma'_2)\) 为使用 Reach-Maxmargin (带有礼物下界保证) 求解子博弈 S 得到的策略。则最终策略的可利用性满足：
  
        \[
        expl(\sigma') \le expl(\sigma) - \sum_{h \in S_{top}} \pi^\sigma_1(h) M^\sigma_1(I_1(h))
        \]

        这里的 \(M^\sigma_1(I_1(h)) = CBV^\sigma(I_1(h)) - v^\sigma(I_1(h))\) 是 **P1 在原始蓝图策略 \(\sigma\) 下**，在包含入口节点 \(h\) 的信息集 \(I_1(h)\) 上的边际值。\(\pi^\sigma_1(h)\) 是 P1 的到达概率。
        
    *   **重要意义:** 这个定理表明，Reach 方法不仅是安全的 (因为 \(M^\sigma_1 \ge 0\)，所以 \(expl(\sigma') \le expl(\sigma)\))，而且如果 P1 在原始蓝图策略的子博弈边界上存在正的边际值（即 \(\sigma_1\) 在这些入口信息集上不是最佳响应），那么 Reach 方法有潜力**严格地降低**全局策略的可利用性。改进量与原始蓝图策略在边界上的"弱点"（边际值）的大小有关。

## 第 6 章：替代收益的估计 (Estimates for Alternative Payoffs)

*   **问题:** Resolve 方法需要精确计算 P1 的退出收益 \(v^\sigma_1(h)\) (等价于 P2 的 \(v^\sigma_2(h)\))。Reach-Maxmargin 需要计算精确的 \(CBV^{\sigma'}(I_1)\) 和 \(v^{\sigma'}(I_1)\) 来确定边际值。在大型博弈中，精确计算这些值可能需要遍历蓝图策略 \(\sigma\) 的很大部分（甚至整个游戏树），计算成本非常高，违背了子博弈求解希望"实时"进行的初衷。

*   **核心思想:** 在计算蓝图策略 \(\sigma\) 时（通常使用 CFR 算法），算法本身会维护和更新各个信息集上的**反事实值 (CFV) 的估计**。我们能否直接**重用这些在计算蓝图时得到的估计值** \(\hat{v}^\sigma(I)\), \(\hat{v}^\sigma(I, a)\) （以及由此导出的 \(\widehat{CBV}^\sigma(I)\)），来代替需要精确计算的替代收益？

*   **基于估计的方法:**
    *   **Estimate (估计):** 在类似 Resolve 的框架中进行修改。当 P1 在增广子博弈的入口点 \(I_1 \in S_{top}\) 考虑"退出"时，其退出收益不再是精确的 \(v^\sigma_1(I_1)\)，而是使用**P2 在该信息集的估计反事实最佳值 (Estimated CBV for P2)** \(\widehat{CBV}^\sigma_2(I_1)\) 作为替代。（因为是零和博弈，P1 的收益是 \(-\widehat{CBV}^\sigma_2(I_1)\)）。然后在这个设定下求解增广子博弈的 NE，得到 \(\sigma'_2\)。
    *   **Estimate + Distributional (估计 + 分布式):** 仅仅使用平均估计值可能不够鲁棒。CFR 迭代过程不仅可以得到值的估计，还可以得到估计的置信度或分布信息（例如标准差）。可以使用这些分布信息（如 Appendix B.1 中描述的启发式方法，例如使用均值减去/加上一定倍数的标准差）来调整用作替代收益的估计值，使其更保守或更乐观，以期获得更好的实践性能。
    *   **Estimate + Reach (估计 + Reach):** 将估计值与 Reach-Maxmargin 的目标结合。在计算和优化边际值 \(M^{\sigma'}(I_1)\) 时，使用估计的反事实值 \(\hat{v}^{\sigma'}(I_1, a)\) 来计算 \(\widehat{CBV}^{\sigma'}(I_1)\) 和 \(\hat{v}^{\sigma'}(I_1)\)。

*   **理论保证 (定理 2 - Theorem 2):**
    *   使用估计值通常会**失去 Resolve 的严格安全性保证**。也就是说，\(expl(\sigma')\) 可能大于 \(expl(\sigma)\)。
    *   但是，可利用性的增加量是**有界的**。令 \(\sigma'\) 是使用估计值 \(\hat{v}\) 得到的策略，\(\sigma\) 是蓝图策略。令 \(\Delta\) 为在子博弈 S 边界上（即所有 \(I_1 \in S_{top}\)） P1 的信息集中，真实 CBV 与估计 CBV 之间（针对 P2）的最大绝对误差：
  
        \[
        \Delta = \max_{I_1 \in S_{top}} |CBV^\sigma_2(I_1) - \hat{v}^\sigma_2(I_1)|
        \]
        则定理 2 表明：
        \[
        expl(\sigma') \le expl(\sigma) + 2\Delta
        \]
        
    *   **意义:** 使用估计值导致的可利用性增加量，最多是边界上最大估计误差的两倍。如果 CFR 产生的估计值足够准确（即 \(\Delta\) 很小），那么使用估计值带来的性能损失也是有限的，这为在实践中使用计算成本低得多的估计方法提供了理论依据。

## 第 7 章：嵌套子博弈求解 (Nested Subgame Solving)

*   **问题:** 前面的子博弈求解方法都假设游戏进程严格遵循蓝图策略中定义的行动抽象。但如果对手 P1 在某个信息集 \(I_1\) 采取了一个动作 \(a^*\) ，而这个动作 \(a^*\) **不在**我们构建蓝图策略 \(\sigma\) 时使用的**行动抽象 (Action Abstraction)** 之中怎么办？（例如，蓝图只考虑了下注 1/2 底池和 1 倍底池，但对手实际下注了 3/4 底池）。标准的子博弈求解无法直接处理这种情况，因为它无法将 \(a^*\) 映射到增广子博弈的某个入口。

*   **解决方案 (行动转换 Action Translation):**
    1.  **动作映射:** 当 P1 在信息集 \(I_1\) 采取了抽象外的动作 \(a^*\) 时，首先将这个动作 \(a^*\) **映射**到蓝图中该信息集 \(I_1\) 定义的**抽象动作集合 \(A(I_1)\) 中"最接近"的一个或多个动作 \(a\)**。 "接近"的定义取决于具体游戏和抽象方式（例如，在扑克中通常是下注大小最接近）。如果映射到多个动作，可以用概率加权。
    2.  **子博弈定义:** 正常地，从 P1 采取动作 \(a^*\) 后到达的状态 \(h = h_{parent} \cdot a^*\) 开始，定义一个不完美信息子博弈 \(S\)。
    3.  **计算替代收益:** 在为子博弈 \(S\) 求解 P2 的策略 \(\sigma'_2\) 时（例如使用 Resolve 或 Reach 或 Estimate），需要为 P1 设置替代收益（如退出收益或用于计算边际值的基准）。关键在于，这里使用的替代收益应该基于**映射到的那个抽象动作 \(a\)**。例如，如果使用 Estimate 方法，P1 在某个边界信息集 \(I'_1 \in S_{top}\) 的退出收益可能是 \(-\widehat{CBV}^{\sigma'}_2(I'_1, a)\)，即与 P2 在 \(I'_1\) 面对来自 P1 的（转换后的）动作 \(a\) 时的估计 CBV 相关。注意，这里的 \(\sigma'\) 是指**当前正在执行的、可能已经经过先前子博弈更新的策略**。
    4.  **求解子博弈:** 使用选定的方法（Resolve, Reach, Estimate 等）和基于映射动作计算出的替代收益来求解子博弈 \(S\)，得到 P2 在 S 内的新策略 \(\sigma''_2\)。
    5.  **更新策略:** 将 \(\sigma''_2\) 更新到当前策略中。

*   **嵌套 (Nested):** 这个过程可能是**嵌套**的。如果在子博弈 \(S\) 内部，对手 P1 再次采取了一个抽象外的动作 \(a^{**}\)，那么会再次进行行动转换（将 \(a^{**}\) 映射到蓝图动作 \(a'\)），定义更深的子博弈 \(S'\)，并基于 \(a'\) 和当前已经应用了 \(\sigma''_2\) 的策略来计算替代收益，然后求解 \(S'\)。

*   **优势:** 嵌套子博弈求解使得 AI 能够对**未预料到的、抽象之外的行动**做出合理且基于博弈论原理的响应，而不需要完全重新计算整个策略或为所有可能的对手行动都预先建立抽象。它巧妙地利用了蓝图中与意外行动"相似"的抽象行动所关联的策略信息（特别是价值估计）。

*   **与礼物概念的联系:** 论文提到，选择替代收益基于映射动作 \(a\) 而非实际动作 \(a^*\) 可以被看作一种"礼物"。例如，\(CBV^{\sigma'}_2(I_1, a) - CBV^{\sigma'}_2(I_1)\)（如果用 CBV 作为基准）可以视为 P1 给予 P2 的礼物，因为 P2 是根据"假设" P1 采取了动作 \(a\) 来计算收益的。

## 第 8 章：实验 (Experiments)

*   **测试平台:** **双人无限注德州扑克 (Heads-Up No-Limit Texas Hold'em, HUNLTH)**，这是不完美信息博弈研究的标准测试平台。

*   **实验 1: 对比不同的子博弈求解技术 (表 1-3):**
    *   **设置:**
        *   首先计算一个基础的 HUNLTH **蓝图策略 (Trunk Strategy)**，使用不同粒度的**信息抽象 (Information Abstraction)**（通过手牌聚类的"桶" buckets 数量来衡量，例如 Small Flop 有 200, 2000, 30000 buckets；Large Flop 有 283.7k, 1.65k, 20k buckets; Turn 有 684.6k, 465.1k, 345.3k buckets 等）。
        *   当游戏进行到特定阶段（**翻牌圈 Flop** 或 **转牌圈 Turn**）时，应用文章中讨论的各种子博弈求解技术来计算该子博弈的"增强"策略。
        *   比较的技术包括：**Trunk** (蓝图本身), **Unsafe**, **Resolve**, **Maxmargin**, **Reach-Maxmargin**, **Estimate**, **Estimate + Distributional**, **Reach-Estimate + Distributional** 等，还有区分礼物是否拆分 (split/not split) 的变体。
    *   **评估指标:** **可利用性 (Exploitability)**，单位是 **mbb/h (milli-big-blinds per hand)**，即每手牌平均能被最优对手赢走多少个千分之一大盲注。这个值越低越好，0 表示纳什均衡。
    *   **主要结果:**
        *   **Unsafe** 表现非常差，尤其是在较精细的抽象下（更多 buckets），其可利用性远高于原始蓝图，验证了其"不安全性"。
        *   **Resolve** 确实是安全的（可利用性不高于蓝图），但通常改进幅度有限。
        *   **Reach-Maxmargin** 和基于 **Estimate** 的变体通常表现最好，能够显著降低可利用性，常常比蓝图低一个数量级。
        *   使用**分布式信息 (Distributional)** 对 Estimate 和 Reach-Estimate 方法有显著提升。
        *   是否**拆分礼物 (Split Gifts)** 对 Reach 变体有影响，不拆分（not split）通常略优。
        *   综合来看，**Reach-Estimate + Distributional (not split)** 在多数实验设置下是性能最佳或接近最佳的方法之一。
        *   子博弈求解的效果在**较粗糙的抽象**下（buckets 较少时）通常更显著。

*   **实验 2: 评估嵌套子博弈求解（处理行动转换）(表 4):**
    *   **设置:** 模拟一个玩家（使用较粗糙的行动抽象，例如只考虑 0.75 倍底池下注）遇到另一个玩家采取了抽象外的行动（例如下注 0.5 倍底池）。
    *   **比较方法:**
        *   **Randomized Pseudo-Harmonic Mapping:** 一种简单的基线方法。
        *   **Nested Resolve, Nested Maxmargin, Nested Reach-Maxmargin:** 本文提出的嵌套求解框架下的几种方法。
        *   **Unsafe (Expensive), Maxmargin (Expensive):** 不使用嵌套，而是假设花费巨大代价为对手的每个具体行动（即使是抽象外的）都预计算了策略或值函数。
    *   **评估指标:** 可利用性 (mbb/h)。
    *   **主要结果:**
        *   嵌套子博弈求解方法 (Resolve, Maxmargin, Reach-Maxmargin) 显著优于简单的基线方法。
        *   这些嵌套方法比计算成本极高的"Expensive"方法更有效率，并且性能相当甚至更好。
        *   **嵌套 Reach-Maxmargin** 再次表现出强大的性能。
        *   结果表明，嵌套子博弈求解是处理抽象外行动的一种有效且实用的方法。

*   **Libratus 与 Brains vs AI 比赛:**
    *   明确指出，在 2017 年 1 月进行的 "Brains vs. AI" 比赛中，击败了四位顶尖人类 HUNLTH 玩家的 AI 程序 **Libratus** 就深度整合并使用了本文提出的先进技术。
    *   特别是，Libratus 使用了带有**分布式礼物下界 (distributed lower bounds on gifts)** 的 **Reach-Maxmargin** 作为其主要的子博弈求解器，并结合了**嵌套子博弈求解**来处理行动转换。
    *   图 5 展示了 Libratus 在为期 20 天、进行了 120,000 手牌的比赛中，对人类顶尖玩家的压倒性优势（最终赢了 147 mbb/h）。这是 AI 在无限注扑克领域首次战胜顶级人类玩家的里程碑事件。

## 第 9 章：结论 (Conclusion)

*   **总结:** 本文针对不完美信息博弈中的子博弈求解问题，提出了改进和创新的方法。回顾了核心挑战（子博弈依赖外部策略），并介绍了主要的贡献：
    *   **安全的子博弈求解:** 提出了保证可利用性不增加的方法 (Resolve) 和能严格减少可利用性的方法 (Reach-Maxmargin)。
    *   **基于估计的方法:** 展示了如何使用 CFR 过程中的估计值来大幅降低子博弈求解的计算成本，同时提供了性能损失的理论界限。
    *   **嵌套子博弈求解:** 开发了处理对手抽象外行动的实用框架。

*   **成果:** 通过在大型基准游戏 HUNLTH 上的大量实验，定量地证明了这些方法的有效性。它们能够显著降低策略的可利用性，并且优于先前的技术。

*   **影响:** 这些技术不仅在理论上有所推进，而且在实践中取得了巨大成功，是 AI Libratus 历史性胜利的关键技术支撑。这项工作首次在大规模不完美信息博弈的子博弈求解背景下，对可利用性进行了细致的测量和比较。
